{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02efd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils,preprocessing,feature_extraction,feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from keras import models,layers\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import nltk\n",
    "import re\n",
    "import transformers\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9d2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcol='title'\n",
    "#fcol='text'\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def runDoc2Vec(train,test,epochs):\n",
    "    gtraintagged=train.apply(lambda r: TaggedDocument (words=tokenize_text(r[fcol]),\n",
    "                                                          tags=[r.Label]),axis=1)\n",
    "    gtesttagged=test.apply(lambda r: TaggedDocument (words=tokenize_text(r[fcol]),\n",
    "                                                        tags=[r.Label]),axis=1)\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    #dbow\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(gtraintagged.values)])\n",
    "    for epoch in range(epochs):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(gtraintagged.values)]), total_examples=len(gtraintagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "    y_train, X_train = vec_for_learning(model_dbow, gtraintagged)\n",
    "    y_test_dbow, X_test = vec_for_learning(model_dbow, gtesttagged)\n",
    "    pipedbow=make_pipeline(StandardScaler(), LogisticRegression(n_jobs=1, C=1e5))\n",
    "    pipedbow.fit(X_train, y_train)\n",
    "    y_pred_dbow = pipedbow.predict(X_test)\n",
    "    #dm\n",
    "    model_dmm=Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "    model_dmm.build_vocab([x for x in tqdm(gtraintagged.values)])\n",
    "    for epoch in range(epochs):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(gtraintagged.values)]), total_examples=len(gtraintagged.values), epochs=1);\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "    y_train, X_train = vec_for_learning(model_dbow, gtraintagged)\n",
    "    y_test_dm, X_test = vec_for_learning(model_dbow, gtesttagged)\n",
    "    pipedbow=make_pipeline(StandardScaler(), LogisticRegression(n_jobs=1, C=1e5))\n",
    "    pipedbow.fit(X_train, y_train)\n",
    "    y_pred_dm = pipedbow.predict(X_test)\n",
    "    #combined\n",
    "    #model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    #model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n",
    "    y_train, X_train = vec_for_learning(new_model, gtraintagged)\n",
    "    y_test_combined, X_test = vec_for_learning(new_model, gtesttagged)\n",
    "    pipecomb=make_pipeline(StandardScaler(), LogisticRegression(n_jobs=1, C=1e5))\n",
    "    pipecomb.fit(X_train, y_train)\n",
    "    y_pred_combined = pipecomb.predict(X_test)\n",
    "    return {\n",
    "        'Accuracy Doc2Vec(DBOW)': accuracy_score(y_test_dbow, y_pred_dbow),\n",
    "        'F1 Doc2Vec(DBOW)': f1_score(y_test_dbow, y_pred_dbow, average='weighted'),\n",
    "        'Accuracy Doc2Vec(DM)': accuracy_score(y_test_dm, y_pred_dm),\n",
    "        'F1 Doc2Vec(DM)': f1_score(y_test_dm, y_pred_dm, average='weighted'),\n",
    "        'Accuracy Doc2Vec(Combined)': accuracy_score(y_test_combined, y_pred_combined),\n",
    "        'F1 Doc2Vec(Combined)':f1_score(y_test_combined, y_pred_combined, average='weighted')}\n",
    "\n",
    "def runtfidf(train,test):\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000,ngram_range=(1,2))\n",
    "    corpus = train[fcol]\n",
    "    vectorizer.fit(corpus)\n",
    "    X_train = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "    y = train[\"Label\"]\n",
    "    X_names = vectorizer.get_feature_names()\n",
    "    p_value_limit = 0.95\n",
    "    dtf_features = pd.DataFrame()\n",
    "    for cat in np.unique(y):\n",
    "        chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame({\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "        dtf_features = dtf_features.sort_values([\"y\",\"score\"],ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "    cf=LogisticRegression(n_jobs=1,C=1e5)\n",
    "    pipe=pipeline.Pipeline([('vectorizer',vectorizer),('classifier',cf)])\n",
    "    pipe['classifier'].fit(X_train,y.values)\n",
    "    X_test=test[fcol].values\n",
    "    y_test=test['Label'].values\n",
    "    pred=pipe.predict(X_test)\n",
    "    return {'Accuracy Tf-Idf':accuracy_score(y_test,pred),'F1 Tf-Idf':f1_score(y_test,pred)}\n",
    "\n",
    "def runbow(train,test):\n",
    "    vectorizer = feature_extraction.text.CountVectorizer(max_features=10000,ngram_range=(1,2))\n",
    "    corpus = train[fcol]\n",
    "    vectorizer.fit(corpus)\n",
    "    X_train = vectorizer.transform(corpus)\n",
    "    dic_vocabulary = vectorizer.vocabulary_\n",
    "    y = train[\"Label\"]\n",
    "    X_names = vectorizer.get_feature_names()\n",
    "    p_value_limit = 0.95\n",
    "    dtf_features = pd.DataFrame()\n",
    "    for cat in np.unique(y):\n",
    "        chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "        dtf_features = dtf_features.append(pd.DataFrame({\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "        dtf_features = dtf_features.sort_values([\"y\",\"score\"],ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "    X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "    cf=LogisticRegression(n_jobs=1,C=1e5)\n",
    "    pipe=pipeline.Pipeline([('vectorizer',vectorizer),('classifier',cf)])\n",
    "    pipe['classifier'].fit(X_train,y.values)\n",
    "    X_test=test[fcol].values\n",
    "    y_test=test['Label'].values\n",
    "    pred=pipe.predict(X_test)\n",
    "    return {'Accuracy BOW':accuracy_score(y_test,pred),'F1 BOW':f1_score(y_test,pred)}\n",
    "\n",
    "def runBERT2(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[fcol],data['Label'],test_size=0.2,random_state=14,stratify=data['Label'].values)\n",
    "    bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "    bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessed_text = bert_preprocess(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "    l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "    l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "    model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "    METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    "    model.compile(optimizer='adam',\n",
    "     loss='binary_crossentropy',\n",
    "     metrics=METRICS)\n",
    "    model.fit(X_train,y_train,epochs=30)\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_pred=y_pred.flatten()\n",
    "    pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    return {'Accuracy BERT':accuracy_score(y_test,pred),'F1 BERT':f1_score(y_test,pred)}\n",
    "\n",
    "def runGloVe(data):\n",
    "    #pre-process\n",
    "    data[fcol]=data[fcol].apply(lambda x: x.lower())\n",
    "    #tokenizer\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(data[fcol].values)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    X = tokenizer.texts_to_sequences(data[fcol].values)\n",
    "    #padding\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences(X,maxlen = 1000, padding = 'post')\n",
    "    #create train and test sets\n",
    "    y=pd.get_dummies(data['Label']).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14,stratify=y)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    #load embeddings\n",
    "    embeddings_index = dict()\n",
    "    f = open('./dataset/glove.6B.200d.txt',encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    #create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, 200))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    #model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=1000, trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    y_pred=model.predict(X_test)\n",
    "    #y_pred=y_pred.flatten()\n",
    "    pred = np.where(y_pred > 0.5, True, False)\n",
    "    return {'Accuracy GloVe':accuracy_score(y_test,pred),'F1 GloVe':f1_score(y_test,pred,pos_label=True,average='weighted')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43f3937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18948\n",
      "59484\n",
      "1488\n",
      "1820\n"
     ]
    }
   ],
   "source": [
    "#initialize datasets\n",
    "\n",
    "#nltk.download(\"popular\")\n",
    "file_cols=['id','title','text']\n",
    "gossipli=[]\n",
    "gossipli.append(pd.read_csv('./dataset/gossipcop_fake.csv',index_col=None,usecols=file_cols).assign(Label=False))\n",
    "gossipli.append(pd.read_csv('./dataset/gossipcop_real.csv',index_col=None,usecols=file_cols).assign(Label=True))\n",
    "gossip=pd.concat(gossipli,axis=0,ignore_index=True)\n",
    "politili=[]\n",
    "politili.append(pd.read_csv('./dataset/politifact_fake.csv',index_col=None,usecols=file_cols).assign(Label=False))\n",
    "politili.append(pd.read_csv('./dataset/politifact_real.csv',index_col=None,usecols=file_cols).assign(Label=True))\n",
    "politi=pd.concat(politili,axis=0,ignore_index=True)\n",
    "print(gossip[gossip[\"Label\"]==False].size)\n",
    "print(gossip[gossip[\"Label\"]==True].size)\n",
    "print(politi[politi[\"Label\"]==False].size)\n",
    "print(politi[politi[\"Label\"]==True].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f860277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "gossip[fcol]=gossip[fcol].apply(cleanText)\n",
    "politi[fcol]=politi[fcol].apply(cleanText)\n",
    "gossip_train, gossip_test =train_test_split(gossip,test_size=0.2,random_state=14,stratify=gossip['Label'].values)\n",
    "politi_train, politi_test =train_test_split(politi,test_size=0.2,random_state=14,stratify=politi['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72143e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35384/682507177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#runs gossipcop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgtfidfres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mruntfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgossip_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgossip_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgbowres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrunbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgossip_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgossip_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgdoc2vecres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrunDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgossip_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgossip_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35384/2228242756.py\u001b[0m in \u001b[0;36mruntfidf\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mdic_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1868\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1870\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1871\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m_word_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[1;34m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;31m# handle stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#runs gossipcop\n",
    "\n",
    "gtfidfres=runtfidf(gossip_train,gossip_test)\n",
    "gbowres=runbow(gossip_train,gossip_test)\n",
    "gdoc2vecres=runDoc2Vec(gossip_train,gossip_test,100)\n",
    "ggloveres=runGloVe(gossip)\n",
    "gBERTres=runBERT2(gossip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs politifact\n",
    "\n",
    "ptfidfres=runtfidf(politi_train,politi_test)\n",
    "pbowres=runbow(politi_train,politi_test)\n",
    "pdoc2vecres=runDoc2Vec(politi_train,politi_test,100)\n",
    "pgloveres=runGloVe(politi)\n",
    "pBERTres=runBERT2(politi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7718df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##print Gossipcop results\n",
    "print('----------=================Gossip Results==================----------------')\n",
    "print('Results for Tf-Idf ',gtfidfres)\n",
    "print('Results for BOW ',gbowres)\n",
    "print('Results for Doc2Vec ',gdoc2vecres)\n",
    "print('Results for GloVe',ggloveres)\n",
    "print('Results for BERT ',gBERTres)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cded2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##print Gossipcop results\n",
    "print('----------=================Politi Results==================----------------')\n",
    "print('Results for Tf-Idf ',ptfidfres)\n",
    "print('Results for BOW ',pbowres)\n",
    "print('Results for Doc2Vec ',pdoc2vecres)\n",
    "print('Results for Glove ',pgloveres)\n",
    "print('Results for BERT ',pBERTres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
